{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683a1bb0",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c0937f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ded79a",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09ebe4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Reviews.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b712aae",
   "metadata": {},
   "source": [
    "### First Few Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd5a395a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1698e51a",
   "metadata": {},
   "source": [
    "### Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "631e2c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   Id                      568454 non-null  int64 \n",
      " 1   ProductId               568454 non-null  object\n",
      " 2   UserId                  568454 non-null  object\n",
      " 3   ProfileName             568428 non-null  object\n",
      " 4   HelpfulnessNumerator    568454 non-null  int64 \n",
      " 5   HelpfulnessDenominator  568454 non-null  int64 \n",
      " 6   Score                   568454 non-null  int64 \n",
      " 7   Time                    568454 non-null  int64 \n",
      " 8   Summary                 568427 non-null  object\n",
      " 9   Text                    568454 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 43.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b40aa",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c6f09e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60b4f96",
   "metadata": {},
   "source": [
    "### Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0b869d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05811e5b",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b329c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>568454.000000</td>\n",
       "      <td>568454.000000</td>\n",
       "      <td>568454.00000</td>\n",
       "      <td>568454.000000</td>\n",
       "      <td>5.684540e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>284227.500000</td>\n",
       "      <td>1.743817</td>\n",
       "      <td>2.22881</td>\n",
       "      <td>4.183199</td>\n",
       "      <td>1.296257e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>164098.679298</td>\n",
       "      <td>7.636513</td>\n",
       "      <td>8.28974</td>\n",
       "      <td>1.310436</td>\n",
       "      <td>4.804331e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.393408e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>142114.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.271290e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>284227.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.311120e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>426340.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.332720e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>568454.000000</td>\n",
       "      <td>866.000000</td>\n",
       "      <td>923.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.351210e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Id  HelpfulnessNumerator  HelpfulnessDenominator  \\\n",
       "count  568454.000000         568454.000000            568454.00000   \n",
       "mean   284227.500000              1.743817                 2.22881   \n",
       "std    164098.679298              7.636513                 8.28974   \n",
       "min         1.000000              0.000000                 0.00000   \n",
       "25%    142114.250000              0.000000                 0.00000   \n",
       "50%    284227.500000              0.000000                 1.00000   \n",
       "75%    426340.750000              2.000000                 2.00000   \n",
       "max    568454.000000            866.000000               923.00000   \n",
       "\n",
       "               Score          Time  \n",
       "count  568454.000000  5.684540e+05  \n",
       "mean        4.183199  1.296257e+09  \n",
       "std         1.310436  4.804331e+07  \n",
       "min         1.000000  9.393408e+08  \n",
       "25%         4.000000  1.271290e+09  \n",
       "50%         5.000000  1.311120e+09  \n",
       "75%         5.000000  1.332720e+09  \n",
       "max         5.000000  1.351210e+09  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df6a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72d95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d767d23",
   "metadata": {},
   "source": [
    "## Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e698b9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                         0\n",
       "ProductId                  0\n",
       "UserId                     0\n",
       "ProfileName               26\n",
       "HelpfulnessNumerator       0\n",
       "HelpfulnessDenominator     0\n",
       "Score                      0\n",
       "Time                       0\n",
       "Summary                   27\n",
       "Text                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2355a935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d852b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f32cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0eb5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45389fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d702c0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0839f690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5ce2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f86960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea4905d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a81a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07491d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f69a14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4771351",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7186b2",
   "metadata": {},
   "source": [
    "### Handling Negation Properly\n",
    "Instead of outright removing such words, you can process them to preserve their sentiment-altering role. Here's how:\n",
    "\n",
    "- a. Keep Negations Intact\n",
    "Retain negation words during preprocessing to ensure their influence on sentiment is captured.\n",
    "\n",
    "- b. Negation Handling with Adjacent Words\n",
    "You can append the negation word to the following term (e.g., not good â†’ not_good). This helps models treat it as a unique sentiment word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9e5c83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeshw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  I have bought several of the Vitality canned d...   \n",
      "1  Product arrived labeled as Jumbo Salted Peanut...   \n",
      "2  This is a confection that has been around a fe...   \n",
      "3  If you are looking for the secret ingredient i...   \n",
      "4  Great taffy at a great price.  There was a wid...   \n",
      "\n",
      "                                        Cleaned_Text  \n",
      "0  bought several vitality canned dog food produc...  \n",
      "1  arrived labeled jumbo salted peanutsthe peanut...  \n",
      "2  confection around centuries light pillowy citr...  \n",
      "3  looking secret ingredient robitussin believe f...  \n",
      "4  taffy price wide assortment yummy taffy delive...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the negation handling function\n",
    "def handle_negations(text):\n",
    "    # Replace specific negation contractions with expanded forms\n",
    "    text = re.sub(r\"\\b(can't|cannot)\\b\", \"can_not\", text)\n",
    "    text = re.sub(r\"\\b(won't)\\b\", \"will_not\", text)\n",
    "    text = re.sub(r\"\\b(don't)\\b\", \"do_not\", text)\n",
    "    text = re.sub(r\"\\b(doesn't)\\b\", \"does_not\", text)\n",
    "    text = re.sub(r\"\\b(didn't)\\b\", \"did_not\", text)\n",
    "    text = re.sub(r\"\\b(haven't)\\b\", \"have_not\", text)\n",
    "    text = re.sub(r\"\\b(hadn't)\\b\", \"had_not\", text)\n",
    "    text = re.sub(r\"\\b(wouldn't)\\b\", \"would_not\", text)\n",
    "    text = re.sub(r\"\\b(shouldn't)\\b\", \"should_not\", text)\n",
    "    text = re.sub(r\"\\b(mustn't)\\b\", \"must_not\", text)\n",
    "    text = re.sub(r\"\\b(mightn't)\\b\", \"might_not\", text)\n",
    "    text = re.sub(r\"\\b(needn't)\\b\", \"need_not\", text)\n",
    "\n",
    "    # Combine general \"not\" with the next word if applicable\n",
    "    text = re.sub(r\"\\b(not|no|nor)\\s+(\\w+)\", r\"not_\\2\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Define the main text cleaning function\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Apply negation handling before removing punctuation\n",
    "    text = handle_negations(text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Tokenize text\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Add custom stop words specific to your dataset\n",
    "    custom_stopwords = set(['product', 'amazon', 'would', 'one', 'also', 'could', 'like', 'get', 'use', 'really', 'good', 'great'])\n",
    "    all_stopwords = stop_words.union(custom_stopwords)\n",
    "    \n",
    "    words = [word for word in words if word not in all_stopwords]\n",
    "    \n",
    "    # Rejoin words\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the cleaning function to the 'Text' column\n",
    "df['Cleaned_Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Preview the cleaned text\n",
    "print(df[['Text', 'Cleaned_Text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bb494c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>arrived labeled jumbo salted peanutsthe peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>taffy price wide assortment yummy taffy delive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
       "      <td>got wild hair taffy ordered five pound bag taf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "      <td>saltwater taffy flavors soft chewy candy indiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
       "      <td>taffy soft chewy flavors amazing definitely re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
       "      <td>right im mostly sprouting cats eat grass love ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "      <td>healthy dog food digestion small puppies dog e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I don't know if it's the cactus or the tequila...</td>\n",
       "      <td>do_not know cactus tequila unique combination ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>One of my boys needed to lose some weight and ...</td>\n",
       "      <td>boys needed lose weight did_not put food floor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>My cats have been happily eating Felidae Plati...</td>\n",
       "      <td>cats happily eating felidae platinum two years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>good flavor! these came securely packed... the...</td>\n",
       "      <td>flavor came securely packed fresh delicious lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Strawberry Twizzlers are my guilty pleasur...</td>\n",
       "      <td>strawberry twizzlers guilty pleasure yummy six...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>My daughter loves twizzlers and this shipment ...</td>\n",
       "      <td>daughter loves twizzlers shipment six pounds h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I love eating them and they are good for watch...</td>\n",
       "      <td>love eating watching tv looking movies not_too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I am very satisfied with my Twizzler purchase....</td>\n",
       "      <td>satisfied twizzler purchase shared others enjo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Twizzlers, Strawberry my childhood favorite ca...</td>\n",
       "      <td>twizzlers strawberry childhood favorite candy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Candy was delivered very fast and was purchase...</td>\n",
       "      <td>candy delivered fast purchased reasonable pric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>My husband is a Twizzlers addict.  We've bough...</td>\n",
       "      <td>husband twizzlers addict weve bought many time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I bought these for my husband who is currently...</td>\n",
       "      <td>bought husband currently overseas loves appare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I can remember buying this candy as a kid and ...</td>\n",
       "      <td>remember buying candy kid quality hasnt droppe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>I love this candy.  After weight watchers I ha...</td>\n",
       "      <td>love candy weight watchers cut back still craving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I have lived out of the US for over 7 yrs now,...</td>\n",
       "      <td>lived us yrs miss twizzlers go back visit some...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  \\\n",
       "0   I have bought several of the Vitality canned d...   \n",
       "1   Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2   This is a confection that has been around a fe...   \n",
       "3   If you are looking for the secret ingredient i...   \n",
       "4   Great taffy at a great price.  There was a wid...   \n",
       "5   I got a wild hair for taffy and ordered this f...   \n",
       "6   This saltwater taffy had great flavors and was...   \n",
       "7   This taffy is so good.  It is very soft and ch...   \n",
       "8   Right now I'm mostly just sprouting this so my...   \n",
       "9   This is a very healthy dog food. Good for thei...   \n",
       "10  I don't know if it's the cactus or the tequila...   \n",
       "11  One of my boys needed to lose some weight and ...   \n",
       "12  My cats have been happily eating Felidae Plati...   \n",
       "13  good flavor! these came securely packed... the...   \n",
       "14  The Strawberry Twizzlers are my guilty pleasur...   \n",
       "15  My daughter loves twizzlers and this shipment ...   \n",
       "16  I love eating them and they are good for watch...   \n",
       "17  I am very satisfied with my Twizzler purchase....   \n",
       "18  Twizzlers, Strawberry my childhood favorite ca...   \n",
       "19  Candy was delivered very fast and was purchase...   \n",
       "20  My husband is a Twizzlers addict.  We've bough...   \n",
       "21  I bought these for my husband who is currently...   \n",
       "22  I can remember buying this candy as a kid and ...   \n",
       "23  I love this candy.  After weight watchers I ha...   \n",
       "24  I have lived out of the US for over 7 yrs now,...   \n",
       "\n",
       "                                         Cleaned_Text  \n",
       "0   bought several vitality canned dog food produc...  \n",
       "1   arrived labeled jumbo salted peanutsthe peanut...  \n",
       "2   confection around centuries light pillowy citr...  \n",
       "3   looking secret ingredient robitussin believe f...  \n",
       "4   taffy price wide assortment yummy taffy delive...  \n",
       "5   got wild hair taffy ordered five pound bag taf...  \n",
       "6   saltwater taffy flavors soft chewy candy indiv...  \n",
       "7   taffy soft chewy flavors amazing definitely re...  \n",
       "8   right im mostly sprouting cats eat grass love ...  \n",
       "9   healthy dog food digestion small puppies dog e...  \n",
       "10  do_not know cactus tequila unique combination ...  \n",
       "11  boys needed lose weight did_not put food floor...  \n",
       "12  cats happily eating felidae platinum two years...  \n",
       "13  flavor came securely packed fresh delicious lo...  \n",
       "14  strawberry twizzlers guilty pleasure yummy six...  \n",
       "15  daughter loves twizzlers shipment six pounds h...  \n",
       "16  love eating watching tv looking movies not_too...  \n",
       "17  satisfied twizzler purchase shared others enjo...  \n",
       "18  twizzlers strawberry childhood favorite candy ...  \n",
       "19  candy delivered fast purchased reasonable pric...  \n",
       "20  husband twizzlers addict weve bought many time...  \n",
       "21  bought husband currently overseas loves appare...  \n",
       "22  remember buying candy kid quality hasnt droppe...  \n",
       "23  love candy weight watchers cut back still craving  \n",
       "24  lived us yrs miss twizzlers go back visit some...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"Text\", \"Cleaned_Text\"]].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8e6782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ac6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb557a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeshw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  I have bought several of the Vitality canned d...   \n",
      "1  Product arrived labeled as Jumbo Salted Peanut...   \n",
      "2  This is a confection that has been around a fe...   \n",
      "3  If you are looking for the secret ingredient i...   \n",
      "4  Great taffy at a great price.  There was a wid...   \n",
      "\n",
      "                                        Cleaned_Text  \n",
      "0  bought several vitality canned dog food produc...  \n",
      "1  arrived labeled jumbo salted peanutsthe peanut...  \n",
      "2  confection around centuries light pillowy citr...  \n",
      "3  looking secret ingredient robitussin believe f...  \n",
      "4  taffy price wide assortment yummy taffy delive...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a function for text cleaning\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Tokenize text\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Add custom stop words specific to your dataset\n",
    "    custom_stopwords = set(['product', 'amazon', 'would', 'one', 'also', 'could', 'like', 'get', 'use', 'really', 'good', 'great'])\n",
    "    all_stopwords = stop_words.union(custom_stopwords)\n",
    "    \n",
    "    words = [word for word in words if word not in all_stopwords]\n",
    "    \n",
    "    # Rejoin words\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the cleaning function to the 'Text' column\n",
    "df['Cleaned_Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Preview the cleaned text\n",
    "print(df[['Text', 'Cleaned_Text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91af3693",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "Applying lemmatization can help reduce words to their base forms, which can further improve the quality of the word cloud and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac3d6338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yeshw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\yeshw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  I have bought several of the Vitality canned d...   \n",
      "1  Product arrived labeled as Jumbo Salted Peanut...   \n",
      "2  This is a confection that has been around a fe...   \n",
      "3  If you are looking for the secret ingredient i...   \n",
      "4  Great taffy at a great price.  There was a wid...   \n",
      "\n",
      "                                        Cleaned_Text  \n",
      "0  bought several vitality canned dog food produc...  \n",
      "1  arrived labeled jumbo salted peanutsthe peanut...  \n",
      "2  confection around century light pillowy citrus...  \n",
      "3  looking secret ingredient robitussin believe f...  \n",
      "4  taffy price wide assortment yummy taffy delive...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet data if not already downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text_lemmatization(text):\n",
    "    # Previous cleaning steps\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.strip()\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stopwords = set(['product', 'amazon', 'would', 'one', 'also', 'could', 'like', 'get', 'use', 'really', 'good', 'great'])\n",
    "    all_stopwords = stop_words.union(custom_stopwords)\n",
    "    words = [word for word in words if word not in all_stopwords]\n",
    "    \n",
    "    # Lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the cleaning function with lemmatization\n",
    "df['Cleaned_Text'] = df['Text'].apply(clean_text_lemmatization)\n",
    "\n",
    "# Preview the cleaned text\n",
    "print(df[['Text', 'Cleaned_Text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9ea5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5013e39c",
   "metadata": {},
   "source": [
    "### Benefits of Keeping Negations\n",
    "- Improved Model Performance:\n",
    "Retaining negations preserves the polarity of the sentiment, improving sentiment classification accuracy.\n",
    "- Enhanced Word Embeddings:\n",
    "Models like Word2Vec or TF-IDF can capture \"not_good\" as a single term, distinct from \"good\".\n",
    "- BERT and Advanced Models:\n",
    "Context-aware models like BERT naturally handle negations better when they are kept in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef705931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e89e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f615037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Combine all cleaned text\n",
    "all_words = ' '.join(df['Cleaned_Text']).split()\n",
    "\n",
    "# Get word frequencies\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Define frequency thresholds\n",
    "min_freq = 5  # Adjust based on your dataset\n",
    "max_freq = 10000\n",
    "\n",
    "# Filter words in 'Cleaned_Text'\n",
    "def filter_freq_words(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if min_freq <= word_freq[word] <= max_freq]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "df['Cleaned_Text'] = df['Cleaned_Text'].apply(filter_freq_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801053e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a017e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
